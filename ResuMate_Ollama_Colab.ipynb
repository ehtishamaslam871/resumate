{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb320648",
   "metadata": {},
   "source": [
    "# üéØ ResuMate ‚Äî Ollama LLM Server on Google Colab\n",
    "\n",
    "This notebook runs **Ollama with LLaMA 3.2** on Google Colab's free GPU and exposes it via **ngrok** so your local ResuMate project can use it.\n",
    "\n",
    "### Steps:\n",
    "1. **Run Cell 1** ‚Äî Install Ollama & ngrok\n",
    "2. **Run Cell 2** ‚Äî Start Ollama & pull the model\n",
    "3. **Run Cell 3** ‚Äî Expose Ollama via ngrok (copy the URL!)\n",
    "4. **Paste the ngrok URL** in your local `.env` file\n",
    "\n",
    "### ‚ö†Ô∏è Before Starting:\n",
    "- Go to **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**\n",
    "- Get a free ngrok auth token from https://dashboard.ngrok.com/signup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa3d29f",
   "metadata": {},
   "source": [
    "## Cell 1: Install Ollama & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72817674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install zstd (required for Ollama extraction)\n",
    "!apt-get update -y\n",
    "!apt-get install -y zstd curl ca-certificates\n",
    "\n",
    "# Install Ollama on Colab\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Install ngrok for exposing the server\n",
    "!pip install pyngrok -q\n",
    "\n",
    "print(\"\\n‚úÖ Ollama and ngrok installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cf227",
   "metadata": {},
   "source": [
    "## Cell 2: Start Ollama Server & Pull Model\n",
    "\n",
    "This starts the Ollama server in the background and downloads the LLaMA 3.2:3b model.\n",
    "\n",
    "**First time will take 2-3 minutes** to download the model (~2GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba24f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Find ollama binary (handles PATH issues on Colab)\n",
    "ollama_path = shutil.which(\"ollama\") or \"/usr/local/bin/ollama\"\n",
    "\n",
    "if not os.path.isfile(ollama_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Ollama binary not found! Make sure you ran Cell 1 first.\\n\"\n",
    "        \"If you did, try: Runtime ‚Üí Restart runtime, then re-run Cell 1.\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Found Ollama at: {ollama_path}\")\n",
    "\n",
    "# Start Ollama server in background\n",
    "process = subprocess.Popen(\n",
    "    [ollama_path, \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "print(\"‚è≥ Starting Ollama server...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Pull the model (change to llama3.1:8b if you want better quality)\n",
    "MODEL = \"llama3.2:3b\"  # Options: llama3.2:3b, llama3.2:1b, llama3.1:8b, mistral:7b\n",
    "print(f\"\\nüì• Pulling {MODEL}... (this takes 2-3 min on first run)\")\n",
    "subprocess.run([ollama_path, \"pull\", MODEL], check=True)\n",
    "\n",
    "# Verify\n",
    "print(\"\\nüìã Installed models:\")\n",
    "subprocess.run([ollama_path, \"list\"], check=True)\n",
    "print(f\"\\n‚úÖ {MODEL} is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee07181",
   "metadata": {},
   "source": [
    "## Cell 3: Expose Ollama via ngrok\n",
    "\n",
    "### üîë Get your ngrok auth token:\n",
    "1. Go to https://dashboard.ngrok.com/signup (free account)\n",
    "2. Copy your auth token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "3. Paste it below where it says `YOUR_NGROK_AUTH_TOKEN`\n",
    "\n",
    "### After running this cell:\n",
    "- Copy the **Public URL** printed below\n",
    "- Paste it in your local `model-server/.env` file as `OLLAMA_URL=<paste_here>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c660831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# ‚¨áÔ∏è PASTE YOUR NGROK AUTH TOKEN HERE ‚¨áÔ∏è\n",
    "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN\"  # <-- Replace this!\n",
    "\n",
    "# Set auth token\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Kill any existing tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "# Create tunnel to Ollama (port 11434)\n",
    "public_url = ngrok.connect(11434)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ OLLAMA IS LIVE ON GOOGLE COLAB!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüîó Your Public URL: {public_url}\")\n",
    "print(f\"\\nüìã Copy this and paste in your local .env file:\")\n",
    "print(f\"   OLLAMA_URL={public_url}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Keep this Colab tab OPEN ‚Äî it disconnects after ~90 min of inactivity\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de141615",
   "metadata": {},
   "source": [
    "## Cell 4: Test the Connection (Optional)\n",
    "\n",
    "Run this to verify Ollama is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae78736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Test locally first\n",
    "print(\"üîç Testing Ollama locally...\")\n",
    "try:\n",
    "    resp = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    models = [m['name'] for m in resp.json().get('models', [])]\n",
    "    print(f\"‚úÖ Ollama running! Models: {models}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama not responding: {e}\")\n",
    "\n",
    "# Test a quick generation\n",
    "print(\"\\nüß™ Testing model generation...\")\n",
    "try:\n",
    "    start = time.time()\n",
    "    resp = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"llama3.2:3b\",\n",
    "            \"prompt\": \"Say 'Hello ResuMate!' in one line.\",\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0.1, \"num_predict\": 50}\n",
    "        },\n",
    "        timeout=60\n",
    "    )\n",
    "    elapsed = round(time.time() - start, 2)\n",
    "    result = resp.json().get('response', '')\n",
    "    print(f\"‚úÖ Model response ({elapsed}s): {result.strip()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Generation failed: {e}\")\n",
    "\n",
    "# Test via ngrok (if tunnel is active)\n",
    "print(\"\\nüåê Testing via ngrok tunnel...\")\n",
    "try:\n",
    "    from pyngrok import ngrok\n",
    "    tunnels = ngrok.get_tunnels()\n",
    "    if tunnels:\n",
    "        tunnel_url = tunnels[0].public_url\n",
    "        resp = requests.get(f\"{tunnel_url}/api/tags\", timeout=10)\n",
    "        print(f\"‚úÖ ngrok tunnel working! URL: {tunnel_url}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No ngrok tunnel found. Run Cell 3 first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ngrok test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35263e9",
   "metadata": {},
   "source": [
    "## Cell 5: Keep Alive (Run this to prevent disconnect)\n",
    "\n",
    "Google Colab disconnects after ~90 minutes of inactivity. Run this cell to keep the session alive.\n",
    "\n",
    "**Press the Stop button when you're done using ResuMate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"üîÑ Keep-alive running... (press Stop to end)\")\n",
    "print(\"   This prevents Colab from disconnecting.\\n\")\n",
    "\n",
    "counter = 0\n",
    "while True:\n",
    "    counter += 1\n",
    "    time.sleep(60)  # Ping every 60 seconds\n",
    "    clear_output(wait=True)\n",
    "    print(f\"üîÑ Keep-alive running... ({counter} min elapsed)\")\n",
    "    print(f\"   Session is active. Press Stop button to end.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
